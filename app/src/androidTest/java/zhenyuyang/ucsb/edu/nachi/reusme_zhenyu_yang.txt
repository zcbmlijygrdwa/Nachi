ZHENYU YANG

Goleta, CA 93117
+1 (312) 888-0068
zhenyuyang

-----------------------------------------
Education
University of California, Santa Barbara (UCSB), Santa Barbara
Sep 2015 -  Expected: June 2018
M.S., Electrical and Computer Engineering
GPA: 3.86/4.0
Advisor: Kwang-Ting (Tim) Cheng    
    
Illinois Institute of Technology (IIT), Chicago
Aug 2013 - May 2015
B.S., Electrical Engineering
GPA: 3.92/4.0

North China Electric Power University (NCEPU), Beijing    Sep 2011 - July 2013
B.S., Electrical Engineering
GPA: 3.71/4.0

-----------------------------------------
Experience

Productive Robotics, Carpinteria, CA        Computer Vision Engineer    June 2018 – Sep. 2018
Project: Part locator(C++, Python)
-Implemented object detection based on fused image stream from an RGBD camera to locate metal parts in the 3D workspace
-Calibrated RGBD camera to reduce image distortions and align the camera to the robot space coordinate
-Constructed a ROS server package as a backend servo application to perform picking and placing tasks and optimized speed to meet costumers requirement.
-Implemented point cloud registration to reconstruct the robot workspace and update it with voxel integration to fulfill collision avoidance functionality of robot arm

Continental Advanced Lidar Solutions, Carpinteria, CA        Algorithm Intern    June 2016 – May 2017
Project: Street scene real-time reconstruction in Autonomous Driving(C++ & Matlab)
-Implemented a real-time point cloud viewer to render a street scene filmed by 4 Lidar cameras
-Simulated a virtual Lidar camera to investigate the effect of raindrops on UV rays
-Implemented Fast Point Feature Histogram(FPFH) algorithm and tested with raw point cloud data
-Interfaced Matlab core to pthread programs to boost computation of FPFH by 37%

-----------------------------------------
Research Projects    
Through-the-Lens Drone Filming, UCSB    (November 2017– March 2018)
# A groundbreaking interactive drone control system that makes drone filming like playing a video game
-Made an Android client to display the skeleton detections result and convert user gestures into drone flight control signal
-Established a communication bridge between an Android device and a DJI M100 drone
-Extracted 3D human skeleton poses from 2D RGB camera live stream and calculated the drone's position relative to the skeleton
-Interfaced the tracking system between a drone and an Android application
-Submitted paper “Through-the-Lens Drone Filming”, accepted by IEEE/RSJ International Conference on Intelligent Robots and Systems 2018

Drone Cinematography System for Action Scenes, UCSB    (March 2016 – November 2017)
# An autonomous drone flight program to track objects and achieve desired camera frame composition
-Devised a vision-based feedback control system with an inverse Jacobian control scheme
-Simulated the proposed tracking system in Unity and Gazebo
-Implemented the vision tracking system in Linux ROS with DJI Onboard SDK
-Interfaced the tracking system between a drone and an Android application
-Co-authored a paper “An Autonomous Drone Cinematography System for Action Scenes” accepted by IEEE International Conference on Robotics and Automation 2018

Android-based AR Navigation, UCSB    (January 2016 – March 2016)
# An augmented reality application for street navigation with Vuforia AR library
-Utilized motion sensors, compass, and GPS to obtain the spatial location of the phone
-Built a real-time decoder to process path-points JSON data received from Google Maps APIs 
-Restructured and simplified the app navigation hierarchy based on feedback from 62 users 



-----------------------------------------
Skills
Programming Languages:    C++, Java, Python, Matlab, C#
Tools:    Visual Studio, Matlab, ROS, vim, git, Android Studio, Unity

